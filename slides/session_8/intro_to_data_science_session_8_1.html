<!DOCTYPE html>
<html>
  <head>
    <title>Introduction to Data Science</title>
    <meta charset="utf-8">
    <meta name="author" content="Robert Clements" />
    <link rel="stylesheet" href="rc_css.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# Introduction to Data Science
## Session 8.1
### Robert Clements

---



### Housekeeping

- Final exams (homework) will be cumulative and will be in two parts:

  + Take home portion will be assigned next week in place of a homework assignment (Due by Friday April 6 at noon)
  
  + In class portion will be completed in class on the last day (Apr 3), and due in class by 9:30PM. 

- Any questions?

---
### Session 8.1 Outline

- Statistical learning II
  + Logistic regression
  + *Introduction to Statistical Learning - Chapter 4.3*
---
class: inverse, center, middle
# Statistical Learning II
---
### Classification

Suppose we are trying to predict the medical diagnosis of patients in an emergency room:

- `stroke`
- `drug overdose`
- `epileptic seizure`

given a set of features `\(X\)`.

We want to find either

`$$Y \sim f(X)$$`

or

`$$P(Y = y) \sim f(X)$$`

where `\(Y\)` is a categorical variable, and `\(y\)` are the possible categories.
---
### Question: why not just use Linear Regression?

Why can't we simply encode each category as a number, say:

- 0 = `stroke` 
- 1 = `drug overdose`
- 2 = `epileptic seizure`

and use that as our target and fit a linear regression model and round our predicted value up or down to the nearest category?

---
### Question: why not just use Linear Regression?

Two **main** reasons we shouldn't do this: 

- more than two categories: distance between multiple categories isn't meaningful

- two categories: predictions cannot be interpreted as probabilities
---
### Logistic regression

One of the simplest methods for predicting probabilities for a *binary* target is **logistic regression**. 

`$$P(Y = y | X)$$`

Once you have predicted probabilities, you can perform classification (predicting the category) using probability thresholds.

`$$P(Y = y | X) \geq p \Rightarrow Y = y$$`
---
### Follow along

Either install the `ISLR` package and load the `Default` data set, or download it from Canvas.


```r
library(ISLR)
library(tidyverse)
library(broom)
glimpse(Default)
```

```
## Observations: 10,000
## Variables: 4
## $ default &lt;fct&gt; No, No, No, No, No, No, No, No, No, No, No, No, No, No...
## $ student &lt;fct&gt; No, Yes, No, No, No, Yes, No, Yes, No, No, Yes, Yes, N...
## $ balance &lt;dbl&gt; 729.5265, 817.1804, 1073.5492, 529.2506, 785.6559, 919...
## $ income  &lt;dbl&gt; 44361.625, 12106.135, 31767.139, 35704.494, 38463.496,...
```
---
### The Default data

We want to predict which customers will default on their credit card debt given their credit card balance:

`$$P(\text{default} = \text{Yes} | \text{balance})$$`
---
### Visualize it


```r
Default %&gt;% ggplot(aes(x = default, y = balance)) +
  geom_boxplot()
```

&lt;img src="intro_to_data_science_session_8_1_files/figure-html/unnamed-chunk-2-1.png" style="display: block; margin: auto;" /&gt;
---
### Linear regression


```r
Default &lt;- Default %&gt;% mutate(default_num = case_when(
  default == 'Yes' ~ 1,
  default == 'No' ~ 0
))

lm_mod &lt;- lm(default_num ~ balance, data = Default)

tidy(lm_mod)
```

```
##          term      estimate    std.error statistic       p.value
## 1 (Intercept) -0.0751919588 3.354360e-03 -22.41618 1.262551e-108
## 2     balance  0.0001298722 3.474933e-06  37.37401 2.774969e-286
```
---
### Linear regression


```r
Default &lt;- Default %&gt;% mutate(preds = predict(lm_mod))
Default %&gt;% ggplot(aes(x = balance, y = default_num, color = default)) +
  geom_point() +
  geom_line(aes(y = preds)) +
  labs(y = 'Probabilities') +
  theme_bw()
```

&lt;img src="intro_to_data_science_session_8_1_files/figure-html/unnamed-chunk-4-1.png" style="display: block; margin: auto;" /&gt;
---
### Logistic regression

This is the model we are fitting:

`$$P(Y = y | X) = \frac{e^{\beta_{0} + \beta_{1}X_{1}}}{1+e^{\beta_{0} + \beta_{1}X_{1}}}$$`

We estimate the coefficients using *maximum likelihood*. We will not be covering this here - read the text for details.
---
### Logistic regression

We will use the base R function called `glm`, which stands for *generalized linear model*. It works similarly to `lm`, with one main difference: 

we must specify a model *family*. 


```r
logit_mod &lt;- glm(default ~ balance, data = Default, 
                 family = 'binomial')
```
---
### Aside

Generalized linear models (GLM) is a very broad topic beyond the scope of this course, but basically we can fit *linear* models using different types of target variables (categorical, counts, continuous, exponential, etc.). 

GLM's are more commonly used for statistical inference than for predictive modeling.
---
### Use the `broom` package to look at your model

We see similar model output as what we saw with the linear regression model:

- features
- estimates
- standard error of our estimates
- statistics
- p-values


```r
library(broom)
tidy(logit_mod)
```

```
##          term      estimate    std.error statistic       p.value
## 1 (Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191
## 2     balance   0.005498917 0.0002203702  24.95309 1.976602e-137
```
---
### Make predictions

Get **predicted probabilities** like this:

`$$\hat{P}(\text{default} = \text{Yes} | \text{balance} = 1100) = \frac{e^{0.0055 - 10.65\times1100}}{1 + e^{0.0055 - 10.65\times1100}}$$`

In R, making predictions with GLM's is similar to the linear regression model, with only one difference:

we must specify the *type* of prediction.


```r
predict(logit_mod, newdata = Default, type = 'response')
```
---
### Visualize the predictions


```r
sim_balance &lt;- tibble(seq(min(Default$balance), max(Default$balance), length.out = 1000))
names(sim_balance) &lt;- 'balance'
sim_balance &lt;- sim_balance %&gt;% mutate(preds = 
                                        predict(logit_mod, type = 'response', newdata = sim_balance))
sim_balance %&gt;% ggplot(aes(x = balance, y = preds)) +
  geom_line() +
  geom_point(data = Default, aes(y = default_num, color = default)) +
  labs(y = 'Probabilities') +
  theme_bw()
```

&lt;img src="intro_to_data_science_session_8_1_files/figure-html/unnamed-chunk-8-1.png" style="display: block; margin: auto;" /&gt;
---
### Interpreting coefficients

Although we are using a linear combination of features, the relationship between our features and the predicted probability is not a linear one.

Instead, we can say: an increase in credit card balance is associated with an increased probability of default.

More technically, we would say: *a one dollar increase in the credit card balance increases the log-odds of default by .005*...but then we'd have to talk about log-odds.


```r
tidy(logit_mod)
```

```
##          term      estimate    std.error statistic       p.value
## 1 (Intercept) -10.651330614 0.3611573721 -29.49221 3.623124e-191
## 2     balance   0.005498917 0.0002203702  24.95309 1.976602e-137
```
---
### Logistic regression

Similar to linear regression we can:

- use categorical features in our model and interpret them similarly

`$$P(\text{default} = \text{Yes} | \text{student})$$`

- include more than one feature in the model

`$$P(\text{default} = \text{Yes} | \text{student, balance})$$`

Dissimilar to linear regression we can:

- Have more than two categories in our target variable

---
### Pop quiz

Add `student` to the model and interpret the coefficients.


---
class: inverse, center, middle

# End of Session 8.1
    </textarea>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {window.dispatchEvent(new Event('resize'));});
(function() {var d = document, s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler"); if (!r) return; s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }"; d.head.appendChild(s);})();</script>

<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  tex2jax: {
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre']
  }
});
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
