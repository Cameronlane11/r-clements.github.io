---
title: "Introduction to Data Science"
subtitle: "Session 8.2"
author: "Robert Clements"
date: ""
output:
  xaringan::moon_reader:
    css: rc_css.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", fig.asp=.5, message = FALSE, warning = FALSE)
```

### Session 8.2 Outline

- Statistical learning II
  + Decision trees
  + *Introduction to Statistical Learning - Chapter 8.1*
---
class: inverse, center, middle
# Statistical Learning II
---
### Decision trees

Hopefully you took a look at the awesome [Introduction to Machine Learning](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)

It summarizes 

- the basic workings of decision trees

- training and testing data

- overfitting

---
### Decision trees

Also called Classification and Regression Trees (CART), can do 

- classfication
- regression (surprise, surprise)

There are many variants, but each works essentially the same way - by splitting you data up into smaller buckets, such that the observations in each bucket are very similar to each other, or homogeneous.

---
### Decision trees - regression

Let's look at the `Hitters` data set from the `ISLR` package (or download it from Canvas).

```{r, message = FALSE, warning=FALSE}
library(tidyverse)
library(ISLR)
glimpse(Hitters)
```
---
### `Salary` is our target variable

```{r}
Hitters %>% ggplot(aes(x = Salary)) + 
  geom_histogram() +
  theme_bw()
```
---
### `Salary` is our target variable

```{r}
Hitters <- Hitters %>% mutate(log_salary = log(Salary)) %>% filter(!is.na(log_salary))
Hitters %>% ggplot(aes(x = log_salary)) + 
  geom_histogram() +
  theme_bw()
```
---
### Decision trees - regression

Suppose we want to predict the log of `Salary` based on `Years` and `Hits`.

```{r, warning = FALSE, message = FALSE, fig.asp=1, echo = FALSE}
library(rpart)
library(partykit)
tree_mod <- rpart(log_salary ~ Years + Hits, data = Hitters)
tree_mod1 <- rpart(log_salary ~ Years + Hits, data = Hitters, control = rpart.control(maxdepth = 1))
tree_mod2 <- rpart(log_salary ~ Years + Hits, data = Hitters, control = rpart.control(maxdepth = 2))
tree_mod3 <- rpart(log_salary ~ Years + Hits, data = Hitters, control = rpart.control(maxdepth = 3))
plot(as.party(tree_mod))
```
---
### Decision trees - regression

The **predicted value** is the average of `Salary` in each leaf node.

```{r, echo = FALSE}
plot(as.party(tree_mod))
```

---
### Decision trees - regression

```{r, echo = FALSE, fig.asp = 1}
hits <- c(0, 50.5, 114, 117.5)
hits_end <- c(117.5, 50.5, 114, 117.5)

years <- c(6, 6, 0, 4.5)
years_end <- c(6, 24, 3.5, 24)

segs <- tibble(hits, hits_end, years, years_end)

Hitters %>% ggplot(aes(x = Hits, y = Years)) +
  geom_point(aes(color = log_salary)) +
  geom_hline(yintercept = 4.5, size = 2, alpha = .5) +
  geom_hline(yintercept = 3.5, size = 2, alpha = .5) +
  geom_segment(data = segs, aes(x=hits, xend=hits_end, y=years, yend=years_end), size = 2, alpha = .5) +
  theme_bw()
```
---
### Decision trees - regression

We build a regression tree using two steps:

1. We divide the predictor space — that is, the set of possible values for $X$ into $J$ distinct and non-overlapping regions, $R_{1}, R_{2}, ..., R_{J}$.
  + These regions are also called *terminal nodes* or *leafs* of the tree
  + They are the nodes at the very bottom of the tree in the tree diagram

2. For every observation that falls into the region $R_{J}$, we make the same prediction, which is simply the mean of the response values for the training observations in $R_{J}$.

---
### Decision trees - regression

For example:

In our example we ended up with 7 regions based on `Years` and `Hits`. Whichever region an observation lands in will get a predicted value equal to the average value of all of the training observations in that region.


```{r, echo = FALSE}
plot(as.party(tree_mod))
```
---
### Decision trees - regression

For example:

In our example we ended up with 7 regions based on `Years` and `Hits`. Whichever region an observation lands in will get a predicted value equal to the average value of all of the training observations in that region.

```{r, echo = FALSE}
Hitters %>% ggplot(aes(x = Hits, y = Years)) +
  geom_point(aes(color = log_salary)) +
  geom_hline(yintercept = 4.5, size = 2, alpha = .5) +
  geom_hline(yintercept = 3.5, size = 2, alpha = .5) +
  geom_segment(data = segs, aes(x=hits, xend=hits_end, y=years, yend=years_end), size = 2, alpha = .5) +
  theme_bw()
```
---
### How do we construct the trees?

We use *top-down recursive binary splitting*.

1. Start with all observations in one region

2. Check every possible split point, from each feature, for splitting your observations into two regions

3. Choose the split point that **reduces the sum of the squared residuals** the most

4. Repeat 1-3 on each sub-region

5. Keep going until some stopping criterion is reached, for example, until no region contains more than 5 observations
---
### Growing the tree

```{r, echo = FALSE}
plot(as.party(tree_mod1))
```
---
### Growing the tree

```{r, echo = FALSE}
plot(as.party(tree_mod2))
```
---
### Growing the tree

```{r, echo = FALSE}
plot(as.party(tree_mod3))
```
---
### Growing the tree

```{r, echo = FALSE}
plot(as.party(tree_mod))
```
---
### Decision trees - classification

Let's look at the `Heart` data set (download it from Canvas).

```{r, message = FALSE, warning=FALSE}
Heart <- read_csv('../../demos/session_8/Heart.csv')
glimpse(Heart)
```
---
### Decision trees - classification

Our target variable is `AHD`, the presence of heart disease (Yes or No):

```{r}
Heart %>% count(AHD)
```

We have 13 possible features, a mix of quantitative and categorical.

---
### Decision trees in R

Before we look at how decision trees work for classification, let's see how we do decision trees in R. For this we will use the `rpart` package.

```{r, eval = FALSE}
install.packages('rpart')
library(rpart)
```
---
### Decision trees in R

We will use the `rpart` function, which will have familiar looking arguments. First, load your data using `read_csv`:

```{r, eval = FALSE}
library(tidyverse)
Heart <- read_csv('path to csv file')
```

```{r}
tree_mod0 <- rpart(AHD ~ ., data = Heart)
```

What do you think the dot `.` is doing?

---
### Decision trees in R

This is the output from the model object:

```{r}
tree_mod0
```
---
### Decision trees in R

You can look at a summary:

```{r, eval = FALSE}
summary(tree_mod0)
```
---
### Decision trees in R

You can look at an awful plot:

```{r, eval = TRUE}
plot(tree_mod0)
text(tree_mod0, use.n = TRUE)
```
---
### Decision trees in R

Or we can look at much nicer plots using the `partykit` package and converting our model to a `party` object.

```{r}
library(partykit)
plot(as.party(tree_mod0))
```
---
### Classification trees

Classification trees work the same way as regression trees, with two differences:

1. The split points are not chosen by **reducing the sum of the squared residuals**

2. The prediction is not the **mean of the response values**
---
### The gini index and entropy

Recall that when we divide our data into regions, we want each region to be as homogeneous as possible (meaning that we want the target values to be as similar as possible). For categorical target variables, there are two metrics that we can use that try to estimate homogeneity:

gini index: $$\sum_{k = 1}^{K} \hat{p}_{k}(1-\hat{p}_{k})$$

entropy: $$−\sum_{k = 1}^{K} \hat{p}_{k}\log{\hat{p}_{k}}$$

where $\hat{p}_{k}$ is the proportion of observations in the region that belong to class $k$
---
### The gini index and entropy

Both of these are measures of node (or region) purity. Notice that:

The gini index and entropy will both be close to zero if a high proportion of the observations in the region are the same class. 

gini index: $$\sum_{k = 1}^{K} \hat{p}_{k}(1-\hat{p}_{k})$$

entropy: $$−\sum_{k = 1}^{K} \hat{p}_{k}\log{\hat{p}_{k}}$$
---
### Predictions

The prediction for a classification tree can be done in one of two ways:

1. Choose the category that is most prevalent in the terminal node
2. Predict a "probability" using the proportion of observations in the terminal node that are equal to each category

```{r, echo=FALSE}
plot(as.party(tree_mod0))
```
---
### Predictions in R

Predictions for our regression problem using the `Hitters` data set:

```{r}
Hitters <- Hitters %>% 
  mutate(log_salary = log(Salary)) %>% 
  filter(!is.na(log_salary))

tree_mod_reg <- rpart(log_salary ~ Years + Hits, data = Hitters)

preds_reg <- predict(tree_mod_reg, newdata = Hitters)

head(preds_reg)
```
---
### Predictions in R

We can take a look at a plot of our *observed values* versus our *predicted values*:

```{r}
Hitters$preds_reg <- preds_reg
ggplot(data = Hitters, aes(x = preds_reg, y = log_salary)) +
  geom_point() +
  labs(x = 'Predicted log salary', y = 'Actual log salary') +
  theme_bw()
```
---
### Pop quiz

Why the strange patterns?

```{r, echo = FALSE}
ggplot(data = Hitters, aes(x = preds_reg, y = log_salary)) +
  geom_point() +
  labs(x = 'Predicted log salary', y = 'Actual log salary') +
  theme_bw()
```
---
### Predictions in R

And this is what predictions look like from our classification problem using the `Heart` data:

```{r}
preds <- predict(tree_mod0, newdata = Heart)
head(preds)
```
---
### Pros and cons of trees

Pros:

- easy to explain and interpret

- for small trees, easy to visualize

- can handle categorical features easily (linear regression models have to create dummy variables)

- feature distributions don't matter because trees only consider the order of the values

Cons:

- overfit easily, not very robust

- not as accurate as regression models

*both of these cons can be alleviated using more advanced methods, such as a random forest*
---
class: inverse, center, middle

# End of Session 8.2