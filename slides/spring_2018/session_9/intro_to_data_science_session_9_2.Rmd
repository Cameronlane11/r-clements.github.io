---
title: "Introduction to Data Science"
subtitle: "Session 9.2"
author: "Robert Clements"
date: ""
output:
  xaringan::moon_reader:
    css: rc_css.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", fig.asp=.5, message = FALSE, warning = FALSE)
```

### Session 9.2 Outline

- Statistical learning III
  + Bagging - you'll need the `modelr` package
  + Random forest - you'll need the `randomForest` package
  + *Introduction to Statistical Learning - Chapter 8.2.1-8.2.2*
---
class: inverse, center, middle
# Bagging
---
### Decision trees

Decision trees suffer from high *variance* (recall the bias-variance tradeoff)


```{r, eval = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(rpart)
library(partykit)
library(ISLR)
Heart <- read_csv('path to Heart.csv')
Heart <- Heart %>% mutate(AHD = as.factor(AHD), ChestPain = as.factor(ChestPain),
                            Thal = as.factor(Thal))
```

```{r, echo = FALSE, message = FALSE, warning=FALSE}
library(tidyverse)
library(rpart)
library(partykit)
library(ISLR)
Heart <- read_csv('../../demos/session_8/Heart.csv')
Heart <- Heart %>% mutate(AHD = as.factor(AHD), ChestPain = as.factor(ChestPain),
                            Thal = as.factor(Thal))
```

```{r}
model_samp_1 <- Heart %>% sample_frac(.5) 
model_samp_2 <- Heart %>% sample_frac(.5)
model_samp_3 <- Heart %>% sample_frac(.5)
```
---
### Decision trees

```{r}
tree_mod_1 <- rpart(AHD ~ ., data = model_samp_1)
tree_mod_2 <- rpart(AHD ~ ., data = model_samp_2)
tree_mod_3 <- rpart(AHD ~ ., data = model_samp_3)
```
---
### Decision trees

```{r}
plot(as.party(tree_mod_1))
```
---
### Decision trees

```{r}
plot(as.party(tree_mod_2))
```
---
### Decision trees

```{r}
plot(as.party(tree_mod_3))
```
---
### Decision trees

We can grow a tree so that it has very *low bias*

```{r}
deep_tree <- rpart(AHD ~ ., data = Heart, 
                   control = rpart.control(cp = 0, minsplit = 0))
```
---
### Decision trees

```{r, echo = FALSE, fig.asp = 2}
plot(as.party(deep_tree))
```
---
### Bootstrap aggregating

Recall the bootstrap sample:

1. take a sample of size $n$ from your dataset, *with replacement*

2. estimate your parameter of interest (mean, coefficients, etc.), call it $\hat{\alpha}$

3. Repeat Steps 1 and 2, $m$ times, so that we have $m$ estimates of our parameter: $\hat{\alpha}_{1}, \hat{\alpha}_{2}, ...,\hat{\alpha}_{m},$

4. Standard error of $\hat{\alpha}$ is estimated by:

$$SE(\hat{\alpha}) = \text{standard deviation of your set of }\hat{\alpha}_{i}$$
---
### Bootstrap aggregating

We are going to use this idea, but change it slightly:

1. take a sample of size $n$ from your dataset, *with replacement*

2. fit your model of interest (regression, trees, etc.), call it $\hat{f}$

3. Repeat Steps 1 and 2, $m$ times, so that we have $m$ model fits: $\hat{f}_{1}, \hat{f}_{2}, ...,\hat{f}_{m},$

4. Average the predictions from your models:

$$\hat{f}_{avg} = \frac{1}{m}\sum_{i=1}^{m}\hat{f}_{i}(X)$$
---
### Bootstrap aggregating

Bootstrap aggregating, or bagging, is a method for reducing the variance of a statistical learning model by:

1. Training $m$ models on $m$ boostrap samples of your training data

2. Averaging the predictions from all $m$ models

Intuitively, you can probably see how this would reduce the **variance**
---
### Bagging trees

Bagging is especially effective on decision trees due to the very high variance of trees. But, how do you get predictions from bagged trees?

- **Regression**: average the predictions from each individual tree

- **Classification**: majority vote - for each individual tree the predicted class is simply the majority class in the terminal node, then take the most common predicted class over the $m$ trees 

- **Probabilities**: proportion of votes for the positive class
---
### Single decision tree vs bagged trees

Let's split the data into train and test using the `resample_partition()` function from `modelr`

```{r, warning=FALSE, message = FALSE}
library(modelr)
set.seed(9870)
Heart_sets <- resample_partition(Heart, c(test = 0.3, train = 0.7))
class(Heart_sets)
Heart_sets
```
---
### Single decision tree vs bagged trees

Fit and evaluate a single decision tree. We'll use the AUC.

```{r, message = FALSE, warning = FALSE}
library(ROCR) # use ROCR
one_tree <- rpart(AHD ~ ., data = Heart_sets$train) # fit tree
preds <- predict(one_tree, Heart_sets$test)[,2] # get predictions

actuals <- as.tibble(Heart_sets$test)$AHD # extract actuals

pred_obj <- prediction(predictions = preds, 
                       labels = actuals)

one_tree_auc <- performance(pred_obj, measure = 'auc')@y.values[[1]]
one_tree_auc
```
---
### Single decision tree vs bagged trees

Now let's do bagging, with 100 bootstrap samples from the training set, and evaluate the final model on the test set. *Code not shown.*

```{r, echo = FALSE, warning = FALSE, message = FALSE}
library(randomForest)
rf_tr <- as.tibble(Heart_sets$train)
rf_te <- as.tibble(Heart_sets$test)
set.seed(909)
bagged_model <- randomForest(AHD ~ ., data = rf_tr, mtry = 13, ntrees = 100, na.action=na.roughfix)
bagged_preds <- predict(bagged_model, newdata = rf_te, type = 'prob')[,2]
```

```{r}
pred_obj_bagged <- prediction(predictions = bagged_preds, 
                       labels = actuals)

bagged_100_auc <- performance(pred_obj_bagged, measure = 'auc')@y.values[[1]]
bagged_100_auc
```
---
### Single decision tree vs bagged trees

Ok, we saw that the test set AUC was much higher for the bagged trees. Let's see how the test set AUC changes for different numbers of bagged trees. 

Solid line = bagged trees  
Dashed line = single tree

```{r, echo = FALSE}
aucs <- rep(NA, 11)
trees <- c(50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)
for(i in 1:11) {
  set.seed(i*200)
  bagged_model <- randomForest(AHD ~ ., data = rf_tr, mtry = 13, ntrees = trees[i], na.action=na.roughfix)
  bagged_preds <- predict(bagged_model, newdata = rf_te, type = 'prob')[,2]
  pred_obj_bagged <- prediction(predictions = bagged_preds, 
                       labels = actuals)

  aucs[i] <- performance(pred_obj_bagged, measure = 'auc')@y.values[[1]]
}
bagged_aucs <- tibble(trees = trees, auc = aucs)
```

```{r, echo = FALSE}
bagged_aucs %>% ggplot(aes(x = trees, y = auc)) +
  geom_step() +
  geom_hline(yintercept = one_tree_auc, linetype = 2) +
  scale_y_continuous(limits = c(.75, 1)) +
  labs(x = 'Number of trees', y = 'auc', title = 'Test set AUC') +
  theme_bw()
```
---
class: center, middle, inverse
# Random forests
---
### Correlated trees

Bagging produces many trees built on resampled data, but what happens if there are one or two **very strong** features?

- Each tree has a high chance of choosing this strong feature for its first split

- Each tree looks similar to each other

- Averaging a bunch of correlated trees will not improve performance much

---
### Random forests

Random forests are very similar to bagging trees, with one key difference that *decorrelates* the trees:

- at each split point (node), only a random subset of the features are considered as candidates for splitting the node

  + note that a new sample of features is taken for **every** node, it is not the same sample for each node
  
```{r, echo = FALSE}
feats <- tibble(names(Heart %>% select(-AHD)))
names(feats) <- 'features to sample from'
cat('Age, Sex, ChestPain, RestBP, Chol, Fbs, \nRestECG, MaxHR, ExAng, Oldpeak, Slope, Ca, Thal')
```

---
### Random forest tuning parameters

There are primarily two parameters that you have control over that can affect the predictive performance of your final model:

- *Number of features to sample at each node split

- Number of trees to build

**this parameter has the biggest impact assuming you choose a relatively large number of trees to build*
---
### Random forests in R

We'll use the very popular `randomForest` package, which has familiar arguments:

- formula
- data 
- mtry: number of features to sample at each node 
- ntrees: number of trees to build
- na.action: how should missing values be treated?

```{r}
Hearts_train <- as.tibble(Heart_sets$train)
Hearts_test <- as.tibble(Heart_sets$test)
# We have some missing values in our data, so we
# deal with those using na.roughfix
rf_mod <- randomForest(AHD ~ ., data = Hearts_train, 
                       mtry = 3, ntrees = 100, 
                       na.action=na.roughfix)
```
---
### Random forests in R

```{r}
rf_mod
```
---
### Random forests vs bagging and decision tree

```{r, echo=FALSE}
aucs_rf <- rep(NA, 11)
trees <- c(50, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000)
for(i in 1:11) {
  set.seed(i*291)
  rf_model <- randomForest(AHD ~ ., data = rf_tr, mtry = 3, ntrees = trees[i], na.action=na.roughfix)
  rf_preds <- predict(rf_model, newdata = rf_te, type = 'prob')[,2]
  pred_obj_rf <- prediction(predictions = rf_preds, 
                       labels = actuals)

  aucs_rf[i] <- performance(pred_obj_rf, measure = 'auc')@y.values[[1]]
}
rf_aucs <- tibble(trees = trees, auc = aucs_rf)
rf_aucs <- rf_aucs %>% mutate(model = 'random forest')
bagged_aucs <- bagged_aucs %>% mutate(model = 'bagging')
all_aucs <- rbind(rf_aucs, bagged_aucs)
```

```{r, echo = FALSE, fig.asp = .8}
all_aucs %>% ggplot(aes(x = trees, y = auc, color = model)) +
  geom_step() +
  geom_hline(yintercept = one_tree_auc, linetype = 2) +
  scale_y_continuous(limits = c(.75, 1)) +
  labs(x = 'Number of trees', y = 'auc', title = 'Test set AUC') +
  theme_bw()
```
---
### Tuning random forests

How do we select the best value of the `mtry` argument?

---
### But we've lost something

Remember how interpretable decision trees are? And we could even plot them.

Random forests and bagging are what's known as **black box** models - we can't easily peak inside them to see how/why the model works. 

![](pics/blackbox.jpg)
---
### But we've lost something

It doesn't make sense to plot one of the bagged trees from bagging or random forest:

- these trees are usually grown very deep

- one tree has very little relationship with the entire collection of trees

- the reason bagging and random forests works well is because of the *averaging* of trees, and NOT because any subset of the trees are particularly good
---
### Feature importance

We can instead calculate a measure of feature importance, and plot these:

```{r, fig.asp=.7}
rf_mod <- randomForest(AHD ~ ., data = Hearts_train, 
                       mtry = 3, ntrees = 100, 
                       na.action = na.roughfix,
                       importance = TRUE)
varImpPlot(rf_mod, type = 2)
```
---
### Feature importance

- Regression: 
  + for each tree, and for each feature, add up the amount that the residual sum of squares is reduced
  + average over all trees
  + the bigger, the more important the feature is

- Classification: 
  + for each tree, and for each feature, add up the amount that the Gini index is reduced
  + average over all trees
  + the bigger, the more important the feature is 
---
class: inverse, center, middle

# End of Session 9.2