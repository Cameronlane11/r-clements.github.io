---
title: "Introduction to Data Science"
subtitle: "Session 9.1"
author: "Robert Clements"
date: ""
output:
  xaringan::moon_reader:
    css: rc_css.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", fig.asp=.5, message = FALSE, warning = FALSE)
```
### Housekeeping

- Final exam (homework) is cumulative (including today's material) and will be in two parts:

  + Take home portion assigned today in place of a homework assignment (Due by Friday April 6 at noon)
  
  + In class portion will be completed in class on the last day (Apr 3), and due in class by 9:30PM. 
  
  + All course materials will disappear from Canvas on Friday, Apr 6 at midnight. 

- Any questions?

---
### Session 9.1 Outline

- Bootstrapping
  + you'll need either the `ISLR` package or download the `Auto` data from Canvas
- Cross-validation
  + you'll need the `modelr` package
  + *Introduction to Statistical Learning - Chapter 5*
---
class: inverse, center, middle
# Bootstrapping
---
### Recall HW 7, Exercise 7?

One downside of the linear model is that it is sensitive to unusual values because the distance incorporates a squared term. Fit a linear model to the simulated data below, and visualise the results. Rerun a few times to generate different simulated datasets. What do you notice about the model?
---
### What did we notice?

*From the solutions*:

From the distributions of the adjusted R-squared and the slopes, we can see that even when we simulate data from a known model, the slope and the model fit can vary widely simply because of slight changes in the data. This illustrates how sensitive regression can be.

Or, in other words...

this illustrates the **variability** of the linear regression model.
---
### Bootstrapping

Useful for quantifying the variability of an estimate (such as the sample average, for example), or the fitted parameters of a model (like the coefficients of the linear regression model from HW 7).

1. take a sample of size $n$ from your dataset, *with replacement*

2. estimate your parameter of interest (mean, coefficients, etc.), call it $\hat{\alpha}$

3. Repeat Steps 1 and 2, $m$ times, so that we have $m$ estimates of our parameter: $\hat{\alpha}_{1}, \hat{\alpha}_{2}, ...,\hat{\alpha}_{m},$

4. Standard error of $\hat{\alpha}$ is estimated by:

$$SE(\hat{\alpha}) = \text{standard deviation of your set of }\hat{\alpha}_{i}$$
---
### Why do we do *resampling*?

Note that we sample *with replacement*, which means that each time we sample an observation from our data set, we put that observation back so that it might be sampled again.

```{r}
x <- 1:5
samp_no_rep <- sample(x, size = 5)

# sample without replacement = no repeated observations
samp_no_rep

samp_w_rep <- sample(x, size = 5, replace = TRUE)

# sample with replacement = there could be repeated observations
samp_w_rep
```
---
### Why do we do *resampling*?

Note that we sample *with replacement*, which means that each time we sample an observation from our data set, we put that observation back so that it might be sampled again.

We do this because in the real world it is not practical, and often impossible, to get new repeated samples from our population.
---
### Standard error of regression coefficients

We'll use the `Auto` data from the `ISLR` package. You can also download this from Canvas.

```{r, message=FALSE, warning=FALSE}
library(ISLR); library(tidyverse); library(broom)
data('Auto')
glimpse(Auto)
```
---
### Standard error of regression coefficients

We want to predict `mpg` using `horsepower` as our feature.

```{r}
lm_fit <- lm(mpg ~ horsepower, data = Auto)
tidy(lm_fit)
```
---
### Standard error of regression coefficients

Now, let's use bootstrapping to estimate the standard error:

We could use the `boot`, `broom`, or `modelr` package for this, but I want you to see what's happening before you learn the shortcuts

```{r}
data_samp <- Auto %>% sample_frac(1, replace = TRUE)
lm_samp <- lm(mpg ~ horsepower, data = data_samp)
tidy(lm_samp)$estimate
```
---
### Defining R functions

I want to run this code over and over again - this seems like the perfect time to talk about **functions**!!

```{r, eval = FALSE}
data_samp <- Auto %>% sample_frac(1, replace = TRUE)
lm_samp <- lm(mpg ~ horsepower, data = data_samp)
tidy(lm_samp)$estimate
```
---
### Defining R functions

Each function could (but not must) have inputs and outputs:

```{r, eval = FALSE}
# name your function
name_of_function <- function(input arguments) { # define input arguments 
  R code here
  ...
  return(what you want to return) # explicitly return something
}

# call the function
name_of_function(input arguments)

# or assign the output
output_name <- name_of_function(input arguments)
```
---
### Standard error of regression coefficients

Let's define a function that will take a bootstrap sample, fit the linear regression model, and return the coefficients:

```{r}
boot_fun <- function() {
  data_samp <- Auto %>% sample_frac(1, replace = TRUE)
  lm_samp <- lm(mpg ~ horsepower, data = data_samp)
  return(tidy(lm_samp)$estimate)
}

boot_fun()
boot_fun()
```
---
### R `for` loops

I want to call this function over and over again and save the results - this seems like the perfect time to talk about **loops**!

We will only talk about `for` loops
```{r, eval = FALSE}
# create space for the output of your loop 
output_vector <- rep(NA, length of loop)

# start the loop
for(index in some sequence of values) {
  R code here
  ...
  output_vector[index] <- output # save the output for each iteration
}
```
---
### Standard error of regression coefficients

Now call that function 1000 times and collect all of the results:

```{r}
# create empty vectors to save the output
intercepts <- rep(NA, 1000)
slopes <- rep(NA, 1000)

for(i in 1:1000) { # loop over the sequence 1,2,3,...1000
  coefs <- boot_fun()
  intercepts[i] <- coefs[1] # save the output
  slopes[i] <- coefs[2] # save the output
}

# convert to tibble
all_results <- tibble(intercepts, slopes)
all_results
```
---
### Standard error of regression coefficients

Now we compare standard error from our bootstrap estimates to the *theoretical* standard error given by linear regression:

```{r}
# bootstrap standard errors
all_results %>% summarize(se_intercept = sd(intercepts), 
                          se_slope = sd(slopes))

# theoretical standard errors
tidy(lm_fit) %>% select(term, std.error)
```
---
class: inverse, center, middle
# Cross-validation
---
### Training and testing

Up until now we've been splitting up our data into training and testing sets

- Train (fit) the model on the train set

- Test (evaluate) the model on the test set

*Why not just train the model on all of the data and evaluate it?*
---
### Training and testing

Training set error rate << testing set error rate

Meaning: training set error rate will not reveal to us how well our model will perform on future, unseen data!
---
### Training and testing

What do we do if we don't have a test set?

For example: in Kaggle you are given a *training* and *testing* set, but only the *training* data set is useful to you. The *testing* set is not because it doesn't contain the actual target values.


In the homework, we randomly split our data into two sets...
---
### Three approaches

1. Validation set

2. Leave-one-out cross-validation

3. $k$-fold cross-validation
---
### Validation set

![](pics/valid.png)
---
### Validation set
What we've done in the homework:

```{r}
Auto <- Auto %>% mutate(Id = 1:n())

train_set <- Auto %>% sample_frac(.7)

valid_set <- Auto %>% filter(!(Id %in% train_set$Id))

lm_train <- lm(mpg ~ horsepower, data = train_set)

val_preds <- predict(lm_train, newdata = valid_set)

rmse_validation <- sqrt(mean((valid_set$mpg - val_preds)^2))

rmse_validation
```
---
### Pop Quiz

Can you guess what might be a major weakness with the *validation set* approach? 

How could we illustrate that?

---
### Leave-one-out cross-validation

1. Leave out one observation $X_{i}$

2. Fit model on remaining observations $X_{(-i)}$

3. Predict value for left out observation $X_{i}$

4. Repeat Steps 1-3 for all $n$ observations

5. Evaluate the model using the $n$ predicted values
---
### Leave-one-out cross-validation

![](pics/loocv.png)
---
### Leave-one-out cross-validation

Again, we could use one of several packages, but let's do it the hard way:

```{r}
loocv_preds <- rep(NA, nrow(Auto))
for(i in Auto$Id) {
  leave_out <- Auto %>% filter(Id == i)
  train_set <- Auto %>% filter(Id != i)
  lm_train <- lm(mpg ~ horsepower, data = train_set)
  loocv_preds[i] <- predict(lm_train, leave_out)
}

rmse_loocv <- sqrt(mean((Auto$mpg - loocv_preds)^2))

rmse_loocv
```
---
### Pop Quiz

Can you guess what might be a major weakness with the *leave-one-out cross-validation* approach? 
---
### *k*-fold cross-validation

1. Randomly divide the dataset into $k$ groups, or *folds*, of approximately equal size

2. First fold = validation set, remaining $k-1$ folds are the training set

3. Train model on training set, evaluate the model on the validation set

4. Repeat Steps 1-3 on remaining folds

5. This results in $k$ estimates of the test error. Average these to get a final test set error. For example:

$$\text{RMSE}_{\text{total}} = \frac{1}{k} \sum_{i = 1}^{k} \text{RMSE}_{i}$$

---
### *k*-fold cross-validation

![](pics/kfold.png)
---
### *k*-fold cross-validation

Common values of *k* are 5 or 10. This is trickier to code ourselves, so let's use the `crossv_kfold()` function from the `modelr` package:

```{r}
library(modelr)
model_data <- Auto %>% crossv_kfold(k = 5)

model_data
```
---
### A `tibble` with `list` columns

The value returned by the `crossv_kfold()` function is a tibble with three columns:

- train: a column of type `list`

- test: a column of type `list`

- .id: a column of type `character`

This demonstrates the power of `tibbles` in that the columns do not have to be vectors, but can be of different data structures that contain objects.
---
### R `list` structures

A `list` is a generic vector containing other objects.

```{r}
my_list <- list(a = c(1,2,3), 
                b = c('a', 'b', 'c'), 
                c = tibble(x = c(3,2,1), y = c('b', 'd', 'f')))
my_list
```
---
### R `list` structures

Extract elements of a list:

- by name: `$` or `[[]]`
- by position: `[[]]`

```{r}
my_list[['a']]
my_list$a
my_list[[1]]
```

---
### A `tibble` with `list` columns

The `train` and `test` lists contain R `resample` objects, which can be coerced into standard data frames or tibbles.


```{r}
model_data$train[[1]] %>% #extract the first train set
  as.tibble() %>% #convert to tibble
  glimpse() #take a look
```
---
### Next steps

Now that we have our 5 folds, we need to:

1. Train a model using the first element of the `train` list column

2. Evaluate the model, using the rmse, on the first element of the `test` list column

3. Repeat for the remaining 4 folds

4. Take the average of the five rmse's

---
### The `purrr` package

There is a package called `purrr` from the `tidyverse` that we *could* use to iterate over our `train` and `test` list columns.

But, it's a big topic in advanced R programming. If you want to learn more about it, check out these resources:

[http://purrr.tidyverse.org/](http://purrr.tidyverse.org/)

[The iteration chapter from R for Data Science](http://r4ds.had.co.nz/iteration.html)
---
### Instead, we'll use the `for` loop

First, create the empty vector to hold our five RMSE's

```{r, eval = FALSE}
all_rmse <- rep(NA, 5)
```
---
### Instead, we'll use the `for` loop

Then, start the loop
```{r, eval = FALSE}
all_rmse <- rep(NA, 5)
for(i in 1:5) {
  
}
```
---
### Instead, we'll use the `for` loop

Extract the train and test datasets from our list columns using the index `i`

```{r, eval = FALSE}
all_rmse <- rep(NA, 5)
for(i in 1:5) {
  train_data <- model_data$train[[i]] %>% as.tibble()
  test_data <- model_data$test[[i]] %>% as.tibble()
  
}
```
---
### Instead, we'll use the `for` loop

Fit the model on `train_data`, get predictions on `test_data`

```{r, eval = FALSE}
all_rmse <- rep(NA, 5)
for(i in 1:5) {
  train_data <- model_data$train[[i]] %>% as.tibble()
  test_data <- model_data$test[[i]] %>% as.tibble()
  model <- lm(mpg ~ horsepower, data = train_data)
  preds <- predict(model, newdata = test_data)
  
}

```
---
### Instead, we'll use the `for` loop

Calculate the RMSE and save it in the `all_rmse` vector, replacing the NA value

```{r, eval = FALSE}
all_rmse <- rep(NA, 5)
for(i in 1:5) {
  train_data <- model_data$train[[i]] %>% as.tibble()
  test_data <- model_data$test[[i]] %>% as.tibble()
  model <- lm(mpg ~ horsepower, data = train_data)
  preds <- predict(model, newdata = test_data)
  all_rmse[i] <- sqrt(mean((test_data$mpg - preds)^2))
}
```
---
### Instead, we'll use the `for` loop

Take the average of the RMSE values, outside of the loop of course

```{r}
all_rmse <- rep(NA, 5)
for(i in 1:5) {
  train_data <- model_data$train[[i]] %>% as.tibble()
  test_data <- model_data$test[[i]] %>% as.tibble()
  model <- lm(mpg ~ horsepower, data = train_data)
  preds <- predict(model, newdata = test_data)
  all_rmse[i] <- sqrt(mean((test_data$mpg - preds)^2))
}
mean(all_rmse)
```
---
class: inverse, center, middle

# End of Session 9.1