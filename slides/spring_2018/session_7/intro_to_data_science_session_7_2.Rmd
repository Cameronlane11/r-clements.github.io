---
title: "Introduction to Data Science"
subtitle: "Session 7.2"
author: "Robert Clements"
date: ""
output:
  xaringan::moon_reader:
    css: rc_css.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", fig.asp=.5, message = FALSE, warning = FALSE)
```

```{r, warning=FALSE,message=FALSE, echo=FALSE}
library(tidyverse)
```

### Session 7.2 Outline

- Supervised vs unsupervised learning
- Regression vs classification
  + *Introduction to Statistical Learning - Chapter 2.1*
---
class: inverse, center, middle
# Supervised vs unsupervised learning
---
### Supervised vs unsupervised

Most statistical/machine learning problems fall into one of these two categories:

1. Supervised learning

2. Unsupervised learning

---
### Supervised learning

Supervised learning is when, for each observation of a set of features $\huge X$, we have an associated target $\huge Y$.

We hope to model the relationship between our set of features and our target:

$$\huge Y \sim f(X)$$

for either 

(1) inference - to better understand the relationship; or 

(2) prediction - to predict $\huge Y$ given values of $\huge X$

Most of machine learning is devoted to *supervised* learning.
---
### Supervised learning

Supervised learning example:

We've already done this, with the diamonds data set.

$$\Huge price = f(carat, cut)$$
---
### Unsupervised learning

Unsupervised learning is when, for each observation of a set of features $\huge X$, we **do not have** an associated target $\huge Y$.

What kinds of questions can we hope to answer when all we have is $\huge X$?

(1) What are the relationships between the features?

(2) What are the relationships between the observations?
---
### Unsupervised learning

Unsupervised learning example (relationships between observations):

Let's look at the famous `iris` dataset. This data gives 4 measurements (in cm) of sepal and petal length and width for 50 flowers from three species of iris.

```{r}
glimpse(iris)
```
---
### Unsupervised learning

Suppose we didn't already know the Species (because this is an obvious target we could use for supervised learning), and we wanted to know if there are any relationships between our observations of flowers.

If we plot two of our features, do we see any distinct groups?

```{r}
iris %>% ggplot(aes(x = Petal.Width, y = Petal.Length)) +
  geom_point()
```
---
### Unsupervised learning

If we plot two of our features, do we see any distinct groups?

```{r}
iris %>% ggplot(aes(x = Sepal.Width, y = Petal.Length)) +
  geom_point()
```
---
### Unsupervised learning

It seems clear that there are *clusters* of observations here. We can use unsupervised learning to try to identify these clusters. Here, I've plotted the species so you can see that the clusters do exist.

```{r}
iris %>% ggplot(aes(x = Petal.Width, y = Petal.Length, color = Species)) +
  geom_point()
```
---
### Unsupervised learning

Unsupervised learning example (relationships between features):

This is typically referred to as **dimension reduction**, meaning, reducing the number of features we have down to a smaller set of *derived* features, while still retaining as much information as possible.

Principal Components Analysis (PCA) is the classical method for this with a nice visual explanation here: [http://setosa.io/ev/principal-component-analysis/](http://setosa.io/ev/principal-component-analysis/)
---
### Unsupervised learning

.pull-left[
Clustering

- $k$-means (and many variants)
- hierarchical clustering
- density clustering (DBScan)
]
.pull-right[
Dimenstion reduction

- PCA (and many variants)
- linear discriminant analysis
- autoencoders 
]
---
class: inverse, center, middle
# Regression vs Classification
---
### Regression vs Classification

Recall that variables are either *numeric* or *categorical.* This applies to our target variable $\huge Y$ as well, not just our features.

- When $\huge Y$ is numeric, or quantitative, we model $\huge Y \sim f(X)$ using **regression**

- When $\huge Y$ is categorical, we *often* model $\huge Y \sim f(X)$ using **classification**
---
### Regression

Example: we've done this already...

$\huge Y$ = price of diamonds (numeric)

$\huge X$ = (carat, cut, color, etc.) 

$$\Large \text{Price of diamonds} \sim f(\text{carat, cut, color, etc.})$$

---
### Regression vs Classification

The reason we say that we *often* model $\huge Y \sim f(X)$ using **classification** for a categorical target, but not *always* is:

- when our goal is simply to predict the *category* of $\huge Y$, we are doing **classification**

- when our goal is to predict the *probability* that $\huge Y$ is equal to a category, we are actually doing something similar to **regression**
---
### Classification

Example: the `iris` data!

$\huge Y$ = species of iris (categorical)

$\huge X$ = (sepal length, sepal width, petal length, petal width) 

$$\Large \text{Species} \sim f(\text{sepal length, sepal width, etc.})$$

---
### Regression vs Classification

Notes:

- Whether we need to perform regression or classification will help determine what type of model we want to fit

- There are many statistical/machine learning methods that work for both regression **and** classification

- Whether the features are quantitative or categorical doesn't really matter much when deciding which type of model to build
---
### Supervised learning algorithms for regression and classification

.pull-left[
- linear regression
- logistic regression 
- stepwise regression
- generalized linear models
- regularized regression
  + ridge
  + lasso
  + elastic net
- robust regression
  + quantile regression
  + LAD regression
- generalized additive models
]

.pull-right[
- decision trees
- random forests
- boosting
- support vector machines
- neural networks
- deep learning
- and many more
]
---
class: inverse, center, middle

# End of Session 7.2