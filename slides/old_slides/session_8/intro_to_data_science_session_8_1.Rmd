---
title: "Introduction to Data Science"
subtitle: "Session 8.1"
author: "Robert Clements"
date: ""
output:
  xaringan::moon_reader:
    css: rc_css.css
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", fig.asp=.5, message = FALSE, warning = FALSE)
```
### Housekeeping

- Final exams (homework) will be cumulative and will be in two parts:

  + Take home portion will be assigned next week in place of a homework assignment (Due by Friday April 6 at noon)
  
  + In class portion will be completed in class on the last day (Apr 3), and due in class by 9:30PM. 

- Any questions?

---
### Session 8.1 Outline

- Statistical learning II
  + Logistic regression
  + *Introduction to Statistical Learning - Chapter 4.3*
---
class: inverse, center, middle
# Statistical Learning II
---
### Classification

Suppose we are trying to predict the medical diagnosis of patients in an emergency room:

- `stroke`
- `drug overdose`
- `epileptic seizure`

given a set of features $X$.

We want to find either

$$Y \sim f(X)$$

or

$$P(Y = y) \sim f(X)$$

where $Y$ is a categorical variable, and $y$ are the possible categories.
---
### Question: why not just use Linear Regression?

Why can't we simply encode each category as a number, say:

- 0 = `stroke` 
- 1 = `drug overdose`
- 2 = `epileptic seizure`

and use that as our target and fit a linear regression model and round our predicted value up or down to the nearest category?

---
### Question: why not just use Linear Regression?

Two **main** reasons we shouldn't do this: 

- more than two categories: distance between multiple categories isn't meaningful

- two categories: predictions cannot be interpreted as probabilities
---
### Logistic regression

One of the simplest methods for predicting probabilities for a *binary* target is **logistic regression**. 

$$P(Y = y | X)$$

Once you have predicted probabilities, you can perform classification (predicting the category) using probability thresholds.

$$P(Y = y | X) \geq p \Rightarrow Y = y$$
---
### Follow along

Either install the `ISLR` package and load the `Default` data set, or download it from Canvas.

```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(tidyverse)
library(broom)
glimpse(Default)
```
---
### The Default data

We want to predict which customers will default on their credit card debt given their credit card balance:

$$P(\text{default} = \text{Yes} | \text{balance})$$
---
### Visualize it

```{r}
Default %>% ggplot(aes(x = default, y = balance)) +
  geom_boxplot()
```
---
### Linear regression

```{r}
Default <- Default %>% mutate(default_num = case_when(
  default == 'Yes' ~ 1,
  default == 'No' ~ 0
))

lm_mod <- lm(default_num ~ balance, data = Default)

tidy(lm_mod)
```
---
### Linear regression

```{r}
Default <- Default %>% mutate(preds = predict(lm_mod))
Default %>% ggplot(aes(x = balance, y = default_num, color = default)) +
  geom_point() +
  geom_line(aes(y = preds)) +
  labs(y = 'Probabilities') +
  theme_bw()
```
---
### Logistic regression

This is the model we are fitting:

$$P(Y = y | X) = \frac{e^{\beta_{0} + \beta_{1}X_{1}}}{1+e^{\beta_{0} + \beta_{1}X_{1}}}$$

We estimate the coefficients using *maximum likelihood*. We will not be covering this here - read the text for details.
---
### Logistic regression

We will use the base R function called `glm`, which stands for *generalized linear model*. It works similarly to `lm`, with one main difference: 

we must specify a model *family*. 

```{r}
logit_mod <- glm(default ~ balance, data = Default, 
                 family = 'binomial')
```
---
### Aside

Generalized linear models (GLM) is a very broad topic beyond the scope of this course, but basically we can fit *linear* models using different types of target variables (categorical, counts, continuous, exponential, etc.). 

GLM's are more commonly used for statistical inference than for predictive modeling.
---
### Use the `broom` package to look at your model

We see similar model output as what we saw with the linear regression model:

- features
- estimates
- standard error of our estimates
- statistics
- p-values

```{r, message = FALSE, warning = FALSE}
library(broom)
tidy(logit_mod)
```
---
### Make predictions

Get **predicted probabilities** like this:

$$\hat{P}(\text{default} = \text{Yes} | \text{balance} = 1100) = \frac{e^{0.0055 - 10.65\times1100}}{1 + e^{0.0055 - 10.65\times1100}}$$

In R, making predictions with GLM's is similar to the linear regression model, with only one difference:

we must specify the *type* of prediction.

```{r, eval = FALSE}
predict(logit_mod, newdata = Default, type = 'response')
```
---
### Visualize the predictions

```{r}
sim_balance <- tibble(seq(min(Default$balance), max(Default$balance), length.out = 1000))
names(sim_balance) <- 'balance'
sim_balance <- sim_balance %>% mutate(preds = 
                                        predict(logit_mod, type = 'response', newdata = sim_balance))
sim_balance %>% ggplot(aes(x = balance, y = preds)) +
  geom_line() +
  geom_point(data = Default, aes(y = default_num, color = default)) +
  labs(y = 'Probabilities') +
  theme_bw()
```
---
### Interpreting coefficients

Although we are using a linear combination of features, the relationship between our features and the predicted probability is not a linear one.

Instead, we can say: an increase in credit card balance is associated with an increased probability of default.

More technically, we would say: *a one dollar increase in the credit card balance increases the log-odds of default by .005*...but then we'd have to talk about log-odds.

```{r}
tidy(logit_mod)
```
---
### Logistic regression

Similar to linear regression we can:

- use categorical features in our model and interpret them similarly

$$P(\text{default} = \text{Yes} | \text{student})$$

- include more than one feature in the model

$$P(\text{default} = \text{Yes} | \text{student, balance})$$

Dissimilar to linear regression we can:

- Have more than two categories in our target variable

---
### Pop quiz

Add `student` to the model and interpret the coefficients.

```{r, eval = FALSE, echo = FALSE}
logit_mod_2 <- glm(default ~ balance + student, data = Default, family = 'binomial')
tidy(logit_mod_2)
```
---
class: inverse, center, middle

# End of Session 8.1